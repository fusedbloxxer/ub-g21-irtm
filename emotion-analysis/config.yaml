# Task
subtask: "1"

# Training
seed: 42
batch_size: 32
num_workers: 0
prefetch_factor: null

# Model settings
learning_rate: !!float 2e-4
finetune: frozen

# Pretrained Model
model_repo: "cardiffnlp/twitter-roberta-base-emotion"

# Tokenization
max_conv_len: 33
max_uttr_len: 93

# Paths
rootdir: ".."

# JAX settings
gpu_memory: "preallocate"
