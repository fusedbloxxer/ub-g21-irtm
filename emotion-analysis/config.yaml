# Task
subtask: "1"

# Training
seed: 33
batch_size: 2
num_workers: 0
prefetch_factor: null

# Model settings
learning_rate: !!float 1e-4
finetune: head

# Pretrained Model
model_repo: "cardiffnlp/twitter-roberta-base-emotion"

# Tokenization
max_conv_len: 33
max_uttr_len: 93

# Paths
rootdir: ".."

# JAX settings
gpu_memory: "preallocate"
