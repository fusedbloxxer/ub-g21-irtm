{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Backend:  gpu\n",
      "JAX Version:  0.4.23\n",
      "Python:  3.11.0 (main, Oct  5 2023, 23:57:12) [GCC 13.2.1 20230801]\n",
      "System:  posix.uname_result(sysname='Linux', nodename='archlinux', release='6.7.0-arch3-1', version='#1 SMP PREEMPT_DYNAMIC Sat, 13 Jan 2024 14:37:14 +0000', machine='x86_64')\n"
     ]
    }
   ],
   "source": [
    "# Find \".env\" file and add the package to $PATH\n",
    "import os, sys\n",
    "import typing as t\n",
    "from typing import Dict, TypeAlias, Any\n",
    "from dotenv import find_dotenv\n",
    "sys.path.append(os.path.dirname(find_dotenv()))\n",
    "\n",
    "# Use local package for modularity\n",
    "import emotion_analysis as ea\n",
    "from emotion_analysis import config\n",
    "from emotion_analysis.data.dataset import ECACDataset\n",
    "from emotion_analysis.model.trainer import TrainerModule\n",
    "from emotion_analysis.data.loader import DefaultDataLoader\n",
    "from emotion_analysis.model.pretrained import load_text_model\n",
    "from emotion_analysis.model.model import EmotionCauseTextModel\n",
    "from emotion_analysis.data.types import TrainSplit, DataSplit, SubTask\n",
    "from emotion_analysis.data.transform import DataTokenize, DataTransform, DataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrm\n",
    "from jax import Array\n",
    "import jax.tree_util as tree_util\n",
    "from jax.tree_util import tree_structure\n",
    "from jax.typing import ArrayLike, DTypeLike\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax as opt\n",
    "import evaluate as eval\n",
    "import mlflow as mlf\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessors\n",
    "text_encoder_pretrained = load_text_model(config.model_repo, config.cache_dir)\n",
    "tokenizer = text_encoder_pretrained.tokenizer\n",
    "tokenize = DataTokenize( tokenizer,  max_seq_len=config.max_uttr_len)\n",
    "transform = DataTransform(tokenize, max_conv_len=config.max_conv_len)\n",
    "collator = DataCollator(transform)\n",
    "\n",
    "# Load data subsets\n",
    "dataset: Dict[DataSplit, ECACDataset] = ECACDataset.read_data(config.data_dir, config.subtask)\n",
    "ds_train, ds_valid, ds_test = *random_split(dataset['train'], [0.75, 0.25]), dataset['test']\n",
    "num_classes: int = dataset['train'].num_emotions\n",
    "\n",
    "# Train: drop_last=True to avoid JAX graph recompilation\n",
    "dataloader: t.Dict[TrainSplit, DataLoader[t.Dict[str, Array]]] = {\n",
    "    'train': DefaultDataLoader(ds_train,  shuffle=True, collate_fn=collator, drop_last=True),\n",
    "    'valid': DefaultDataLoader(ds_valid, shuffle=False, collate_fn=collator),\n",
    "    'test' : DefaultDataLoader( ds_test, shuffle=False, collate_fn=collator),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jrm.PRNGKey(config.seed)\n",
    "key, trainer_key = jrm.split(key, 2)\n",
    "f1_score = eval.load('f1')\n",
    "\n",
    "trainer = TrainerModule(\n",
    "    key=trainer_key,\n",
    "    finetune=config.finetune,\n",
    "    batch_size=config.batch_size,\n",
    "    max_conv_len=config.max_conv_len,\n",
    "    max_uttr_len=config.max_uttr_len,\n",
    "    text_model_repo=config.model_repo,\n",
    "    learning_rate=config.learning_rate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_until_index(array, index):\n",
    "    output = []\n",
    "    for batch, pad_idx in enumerate(index):\n",
    "        output.append(array[batch, :pad_idx, ...])\n",
    "    output = jnp.concatenate(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 64 :  0.6829146\n",
      "2 64 :  1.8118005\n",
      "3 64 :  1.6454159\n",
      "4 64 :  0.7176057\n",
      "5 64 :  1.1118233\n",
      "6 64 :  1.1784569\n",
      "7 64 :  0.99458706\n",
      "8 64 :  0.9376453\n",
      "9 64 :  0.6109783\n",
      "10 64 :  0.5112756\n",
      "11 64 :  0.77690494\n",
      "12 64 :  0.71182024\n",
      "13 64 :  0.49245372\n",
      "14 64 :  0.8214066\n",
      "15 64 :  0.60007596\n",
      "16 64 :  0.5766284\n",
      "17 64 :  0.68032676\n",
      "18 64 :  0.53054476\n",
      "19 64 :  0.6370242\n",
      "20 64 :  0.5195521\n",
      "21 64 :  0.51246345\n",
      "22 64 :  0.6043467\n",
      "23 64 :  0.4873317\n",
      "24 64 :  0.5514187\n",
      "25 64 :  0.567977\n",
      "26 64 :  0.53544307\n",
      "27 64 :  0.53609014\n",
      "28 64 :  0.53278816\n",
      "29 64 :  0.5270432\n",
      "30 64 :  0.50832355\n",
      "31 64 :  0.7034503\n",
      "32 64 :  0.50238997\n",
      "33 64 :  0.53093904\n",
      "34 64 :  0.5158324\n",
      "35 64 :  0.53135645\n",
      "36 64 :  0.5307175\n",
      "37 64 :  0.4110269\n",
      "38 64 :  0.5268087\n",
      "39 64 :  0.46615025\n",
      "40 64 :  0.5552973\n",
      "41 64 :  0.39142346\n",
      "42 64 :  0.5050903\n",
      "43 64 :  0.531948\n",
      "44 64 :  0.4484294\n",
      "45 64 :  0.6159646\n",
      "46 64 :  0.40936208\n",
      "47 64 :  0.64027905\n",
      "48 64 :  0.4821464\n",
      "49 64 :  0.5448167\n",
      "50 64 :  0.42928627\n",
      "51 64 :  0.5368924\n",
      "52 64 :  0.42956966\n",
      "53 64 :  0.61290574\n",
      "54 64 :  0.26529405\n",
      "55 64 :  0.50270575\n",
      "56 64 :  0.55261576\n",
      "57 64 :  0.5627175\n",
      "58 64 :  0.32770112\n",
      "59 64 :  0.5694393\n",
      "60 64 :  0.63266975\n",
      "61 64 :  0.5807402\n",
      "62 64 :  0.50967056\n",
      "63 64 :  0.60055566\n",
      "64 64 :  0.54157597\n",
      "train 0: {'f1': 0.28026676868288414} f1\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    for i, X in enumerate(dataloader['train']):\n",
    "        # Ignore padded entries\n",
    "        input_mask = X['conv_attn_mask'].sum(axis=1).astype(jnp.int32)\n",
    "\n",
    "        # Forward and backward pass\n",
    "        loss_train, logits, key, trainer.state = trainer.train_step(key, trainer.state, X)\n",
    "\n",
    "        # Track training loss\n",
    "        print(i + 1, len(dataloader['train']), ': ', loss_train)\n",
    "\n",
    "        # Track F1\n",
    "        pred = take_until_index(logits, input_mask).argmax(axis=1)\n",
    "        real = take_until_index(X['emotion_labels'], input_mask)\n",
    "        f1_score.add_batch(predictions=pred, references=real)\n",
    "    print('train {}: {} f1'.format(epoch, f1_score.compute(average='weighted')))\n",
    "\n",
    "    losses = []\n",
    "    for i, X in enumerate(dataloader['valid']):\n",
    "        # Ignore padded entries\n",
    "        input_mask = X['conv_attn_mask'].sum(axis=1).astype(jnp.int32)\n",
    "\n",
    "        # Forward pass\n",
    "        loss_valid, logits, key, _ = trainer.eval_step(key, trainer.state, X)\n",
    "        losses.append(loss_valid)\n",
    "\n",
    "        # Track F1\n",
    "        pred = take_until_index(logits, input_mask).argmax(axis=1)\n",
    "        real = take_until_index(X['emotion_labels'], input_mask)\n",
    "        f1_score.add_batch(predictions=pred, references=real)\n",
    "    print('valid {}: {} f1'.format(epoch, f1_score.compute(average='weighed')))\n",
    "    print('valid {}: {} loss'.format(epoch, np.array(losses).mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-analysis-adVGJZXD-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
