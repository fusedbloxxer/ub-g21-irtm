{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find \".env\" file and add the package to $PATH\n",
    "import os, sys\n",
    "import typing as t\n",
    "from typing import TypeAlias, Any\n",
    "from dotenv import find_dotenv\n",
    "sys.path.append(os.path.dirname(find_dotenv()))\n",
    "\n",
    "# Use local package for modularity\n",
    "import emotion_analysis as ea\n",
    "import emotion_analysis.data.dataset as data\n",
    "import emotion_analysis.data.transform as transform\n",
    "from emotion_analysis.model.emotion_cause_text import load_text_model\n",
    "from emotion_analysis.model.emotion_cause_text import EmotionCauseTextModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrm\n",
    "from jax import Array\n",
    "from jax.typing import ArrayLike, DTypeLike\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax as opt\n",
    "import mlflow as mlf\n",
    "from transformers import RobertaConfig, FlaxRobertaModel, RobertaTokenizerFast\n",
    "from transformers import PretrainedConfig\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.ECACDataset(ea.DATA_DIR, 'task_1', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model along with its tokenizer\n",
    "text_encoder_pretrained = load_text_model()\n",
    "\n",
    "# Select tokenization method\n",
    "batch_size = 32\n",
    "max_sen_len = 93\n",
    "max_doc_len = 33\n",
    "tokenizer = text_encoder_pretrained.tokenizer\n",
    "tokenize = transform.Tokenize(tokenizer, max_length=max_sen_len, padding='max_length')\n",
    "collate = transform.DataTokenizerCollator(tokenize, max_length=max_doc_len)\n",
    "\n",
    "# Load and prepare the data\n",
    "task = 'task_1'\n",
    "DataSplit: TypeAlias = t.Literal['train', 'valid', 'test']\n",
    "ds_test = data.ECACDataset(ea.DATA_DIR, task, split='test')\n",
    "ds_train = data.ECACDataset(ea.DATA_DIR, task, split='train')\n",
    "ds_train, ds_valid = random_split(ds_train, [0.75, 0.25])\n",
    "num_classes: int = ds_test.num_emotions\n",
    "\n",
    "# Split into train-valid-test\n",
    "# Train: drop_last=True to avoid JAX graph recompilation\n",
    "dataloader: t.Dict[DataSplit, DataLoader[t.Dict[str, Array]]] = {\n",
    "    'train': DataLoader(ds_train, batch_size, True, collate_fn=collate, drop_last=True),\n",
    "    'valid': DataLoader(ds_valid, batch_size, False, collate_fn=collate),\n",
    "    'test': DataLoader(ds_test, batch_size, False, collate_fn=collate),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a PR key for init\n",
    "key = jrm.key(ea.SEED)\n",
    "key, init_key = jrm.split(key, 2)\n",
    "\n",
    "# Generate fake data to \"imitate\" a batch\n",
    "fake_input_ids = jnp.zeros((batch_size, max_doc_len * max_doc_len))\n",
    "fake_attn_mask = jnp.zeros_like(fake_input_ids)\n",
    "fake_batch: Any = dict(input_ids=fake_input_ids, attention_mask=fake_attn_mask)\n",
    "\n",
    "# Initialize the model with random params\n",
    "ec_model = EmotionCauseTextModel(text_encoder=text_encoder_pretrained.module, num_classes=num_classes)\n",
    "vars = ec_model.init(init_key, **fake_batch)\n",
    "params = vars['params']\n",
    "\n",
    "# Transfer the pretrained weights \n",
    "params['text_encoder'] = text_encoder_pretrained.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train only the classifier for now...\n",
    "should_freeze = lambda p, _: 'frozen' if 'text_encoder' in p else 'trainable'\n",
    "param_labels = flax.traverse_util.path_aware_map(should_freeze, params)\n",
    "tx = opt.multi_transform({ 'trainable': opt.adamw(2e-4), 'frozen': opt.set_to_zero() }, param_labels)\n",
    "opt_state = tx.init(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-analysis-adVGJZXD-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
