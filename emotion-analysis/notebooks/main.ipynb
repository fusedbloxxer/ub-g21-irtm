{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Backend:  gpu\n",
      "JAX Version:  0.4.23\n",
      "Python:  3.11.0 (main, Oct  4 2023, 22:00:02) [GCC 13.2.1 20230801]\n",
      "System:  posix.uname_result(sysname='Linux', nodename='archlinux', release='6.6.10-arch1-1', version='#1 SMP PREEMPT_DYNAMIC Fri, 05 Jan 2024 16:20:41 +0000', machine='x86_64')\n"
     ]
    }
   ],
   "source": [
    "# Find \".env\" file and add the package to $PATH\n",
    "import os, sys\n",
    "import typing as t\n",
    "from typing import Dict, TypeAlias, Any\n",
    "from dotenv import find_dotenv\n",
    "sys.path.append(os.path.dirname(find_dotenv()))\n",
    "\n",
    "# Use local package for modularity\n",
    "import emotion_analysis as ea\n",
    "from emotion_analysis.data.dataset import ECACDataset\n",
    "from emotion_analysis.data.types import TrainSplit, DataSplit, SubTask\n",
    "from emotion_analysis.data.transform import DataTokenize, DataTransform, DataCollator\n",
    "from emotion_analysis.model.emotion_cause_text import load_text_model\n",
    "from emotion_analysis.model.emotion_cause_text import EmotionCauseTextModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrm\n",
    "from jax import Array\n",
    "import jax.tree_util as tree_util\n",
    "from jax.tree_util import tree_structure\n",
    "from jax.typing import ArrayLike, DTypeLike\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax as opt\n",
    "import mlflow as mlf\n",
    "from transformers import RobertaConfig, FlaxRobertaModel, RobertaTokenizerFast\n",
    "from transformers import PretrainedConfig\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "batch_size = 32\n",
    "max_conv_len = 33\n",
    "max_uttr_len = 93\n",
    "subtask: SubTask = '2'\n",
    "\n",
    "# Load pretrained model along with its tokenizer\n",
    "text_encoder_pretrained = load_text_model()\n",
    "tokenizer = text_encoder_pretrained.tokenizer\n",
    "tokenize = DataTokenize(tokenizer, max_seq_len=max_uttr_len, padding='max_length')\n",
    "\n",
    "# Load data subsets\n",
    "ds_train = ECACDataset(\n",
    "    data_dir=ea.DATA_DIR,\n",
    "    subtask=subtask,\n",
    "    split='train',\n",
    ")\n",
    "ds_test = ECACDataset(\n",
    "    data_dir=ea.DATA_DIR,\n",
    "    subtask=subtask,\n",
    "    split='test',\n",
    ")\n",
    "\n",
    "# Data preprocessors\n",
    "transform: Dict[TrainSplit, DataTransform] = {\n",
    "    'train': DataTransform.from_data(ds_train, tokenize),\n",
    "    'valid': DataTransform.from_data(ds_train, tokenize),\n",
    "    'test': DataTransform.from_data(ds_test, tokenize),\n",
    "}\n",
    "collator: Dict[TrainSplit, DataCollator] = tree_util.tree_map(DataCollator, transform)\n",
    "\n",
    "# Split training into train-valid subsets\n",
    "ds_train, ds_valid = random_split(ds_train, [0.75, 0.25])\n",
    "num_classes: int = len(ds_test.emotions)\n",
    "\n",
    "# Split into train-valid-test\n",
    "# Train: drop_last=True to avoid JAX graph recompilation\n",
    "dataloader: t.Dict[TrainSplit, DataLoader[t.Dict[str, Array]]] = {\n",
    "    'train': DataLoader(ds_train, batch_size, False, collate_fn=collator['train'], drop_last=True),\n",
    "    'valid': DataLoader(ds_valid, batch_size, False, collate_fn=collator['valid']),\n",
    "    'test': DataLoader(ds_test, batch_size, False, collate_fn=collator['test']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a PR key for init\n",
    "key = jrm.key(ea.SEED)\n",
    "key, init_key = jrm.split(key, 2)\n",
    "\n",
    "# Generate fake data to \"imitate\" a batch\n",
    "fake_input_ids = jnp.zeros((batch_size, max_conv_len * max_conv_len))\n",
    "fake_attn_mask = jnp.zeros_like(fake_input_ids)\n",
    "fake_batch: Any = dict(input_ids=fake_input_ids, attention_mask=fake_attn_mask)\n",
    "\n",
    "# Initialize the model with random params\n",
    "ec_model = EmotionCauseTextModel(text_encoder=text_encoder_pretrained.module, num_classes=num_classes)\n",
    "vars = ec_model.init(init_key, **fake_batch)\n",
    "params = vars['params']\n",
    "\n",
    "# Transfer the pretrained weights \n",
    "params['text_encoder'] = text_encoder_pretrained.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train only the classifier for now...\n",
    "should_freeze = lambda p, _: 'frozen' if 'text_encoder' in p else 'trainable'\n",
    "param_labels = flax.traverse_util.path_aware_map(should_freeze, params)\n",
    "tx = opt.multi_transform({ 'trainable': opt.adamw(2e-4), 'frozen': opt.set_to_zero() }, param_labels)\n",
    "opt_state = tx.init(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-analysis-adVGJZXD-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
