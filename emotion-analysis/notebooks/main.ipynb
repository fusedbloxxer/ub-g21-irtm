{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find \".env\" file and add the package to $PATH\n",
    "import os, sys\n",
    "import typing as t\n",
    "from typing import Dict, TypeAlias, Any\n",
    "from dotenv import find_dotenv\n",
    "sys.path.append(os.path.dirname(find_dotenv()))\n",
    "\n",
    "# Use local package for modularity\n",
    "import emotion_analysis as ea\n",
    "from emotion_analysis import config\n",
    "from emotion_analysis.data.dataset import ECACDataset\n",
    "from emotion_analysis.model.trainer import TrainerModule\n",
    "from emotion_analysis.data.loader import DefaultDataLoader\n",
    "from emotion_analysis.model.pretrained import load_text_model\n",
    "from emotion_analysis.model.model import EmotionCauseTextModel\n",
    "from emotion_analysis.data.types import TrainSplit, DataSplit, SubTask\n",
    "from emotion_analysis.data.transform import DataTokenize, DataTransform, DataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrm\n",
    "from jax import Array\n",
    "import jax.tree_util as tree_util\n",
    "from jax.tree_util import tree_structure\n",
    "from jax.typing import ArrayLike, DTypeLike\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax as opt\n",
    "import evaluate as eval\n",
    "import mlflow as mlf\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessors\n",
    "text_encoder_pretrained = load_text_model(config.model_repo, config.cache_dir)\n",
    "tokenizer = text_encoder_pretrained.tokenizer\n",
    "tokenize = DataTokenize( tokenizer,  max_seq_len=config.max_uttr_len)\n",
    "transform = DataTransform(tokenize, max_conv_len=config.max_conv_len)\n",
    "collator = DataCollator(transform)\n",
    "\n",
    "# Load data subsets\n",
    "dataset: Dict[DataSplit, ECACDataset] = ECACDataset.read_data(config.data_dir, config.subtask)\n",
    "ds_train, ds_valid, ds_test = *random_split(dataset['train'], [0.75, 0.25]), dataset['test']\n",
    "num_classes: int = dataset['train'].num_emotions\n",
    "\n",
    "# Train: drop_last=True to avoid JAX graph recompilation\n",
    "dataloader: t.Dict[TrainSplit, DataLoader[t.Dict[str, Array]]] = {\n",
    "    'train': DefaultDataLoader(ds_train,  shuffle=True, collate_fn=collator, drop_last=True),\n",
    "    'valid': DefaultDataLoader(ds_valid, shuffle=False, collate_fn=collator),\n",
    "    'test' : DefaultDataLoader( ds_test, shuffle=False, collate_fn=collator),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jrm.PRNGKey(config.seed)\n",
    "key, trainer_key = jrm.split(key, 2)\n",
    "f1_score = eval.load('f1')\n",
    "accuracy = eval.load('accuracy')\n",
    "\n",
    "trainer = TrainerModule(\n",
    "    key=trainer_key,\n",
    "    finetune=config.finetune,\n",
    "    batch_size=config.batch_size,\n",
    "    max_conv_len=config.max_conv_len,\n",
    "    max_uttr_len=config.max_uttr_len,\n",
    "    text_model_repo=config.model_repo,\n",
    "    learning_rate=config.learning_rate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 64 :  2.2270439\n",
      "2 64 :  1.0621885\n",
      "3 64 :  1.221214\n",
      "4 64 :  1.0962687\n",
      "5 64 :  1.1727551\n",
      "6 64 :  0.6713203\n",
      "7 64 :  0.76793635\n",
      "8 64 :  1.1773005\n",
      "9 64 :  0.7098235\n",
      "10 64 :  0.93583095\n",
      "11 64 :  1.0595444\n",
      "12 64 :  0.84797025\n",
      "13 64 :  0.65686053\n",
      "14 64 :  0.57161474\n",
      "15 64 :  0.6522981\n",
      "16 64 :  0.5666307\n",
      "17 64 :  0.50737184\n",
      "18 64 :  0.5652526\n",
      "19 64 :  0.4539348\n",
      "20 64 :  0.44855458\n",
      "21 64 :  0.4720006\n",
      "22 64 :  0.4528192\n",
      "23 64 :  0.53168285\n",
      "24 64 :  0.4695926\n",
      "25 64 :  0.37161326\n",
      "26 64 :  0.3941302\n",
      "27 64 :  0.5419952\n",
      "28 64 :  0.48950705\n",
      "29 64 :  0.58330864\n",
      "30 64 :  0.7249353\n",
      "31 64 :  0.5716671\n",
      "32 64 :  0.44968382\n",
      "33 64 :  0.667528\n",
      "34 64 :  0.54698396\n",
      "35 64 :  0.6088284\n",
      "36 64 :  0.51612085\n",
      "37 64 :  0.40283164\n",
      "38 64 :  0.5115936\n",
      "39 64 :  0.26780596\n",
      "40 64 :  0.49915668\n",
      "41 64 :  0.47447413\n",
      "42 64 :  0.5111405\n",
      "43 64 :  0.5911601\n",
      "44 64 :  0.56397516\n",
      "45 64 :  0.4524314\n",
      "46 64 :  0.4302904\n",
      "47 64 :  0.48532957\n",
      "48 64 :  0.4806305\n",
      "49 64 :  0.53956836\n",
      "50 64 :  0.4261108\n",
      "51 64 :  0.5737419\n",
      "52 64 :  0.41775984\n",
      "53 64 :  0.48863783\n",
      "54 64 :  0.40412977\n",
      "55 64 :  0.47840992\n",
      "56 64 :  0.41240036\n",
      "57 64 :  0.4837389\n",
      "58 64 :  0.54135853\n",
      "59 64 :  0.48584437\n",
      "60 64 :  0.4500503\n",
      "61 64 :  0.5232113\n",
      "62 64 :  0.40186158\n",
      "63 64 :  0.37507632\n",
      "64 64 :  0.51276416\n",
      "train 0: {'f1': 0.3150494961431431}  f1\n",
      "train 0: {'accuracy': 0.3688862698880377} acc\n",
      "valid 0: 0.483473002910614 loss\n",
      "valid 0: {'f1': 0.3260202449353047}  f1\n",
      "valid 0: {'accuracy': 0.4365315852205006} acc\n",
      "1 64 :  0.41325265\n",
      "2 64 :  0.40720052\n",
      "3 64 :  0.3974973\n",
      "4 64 :  0.3949581\n",
      "5 64 :  0.57547784\n",
      "6 64 :  0.3388897\n",
      "7 64 :  0.43244374\n",
      "8 64 :  0.43887794\n",
      "9 64 :  0.56996804\n",
      "10 64 :  0.5050936\n",
      "11 64 :  0.3790734\n",
      "12 64 :  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m loss_train, logits, key, trainer\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain_step(key, trainer\u001b[38;5;241m.\u001b[39mstate, X)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Track training loss\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataloader[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m, loss_train)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Track F1\u001b[39;00m\n\u001b[1;32m     13\u001b[0m pred \u001b[38;5;241m=\u001b[39m logits[input_mask, :]\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/mnt/storage/.cache/pypoetry/virtualenvs/emotion-analysis-DIRhFH6i-py3.11/lib/python3.11/site-packages/jax/_src/array.py:256\u001b[0m, in \u001b[0;36mArrayImpl.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 256\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m)\n",
      "File \u001b[0;32m/mnt/storage/.cache/pypoetry/virtualenvs/emotion-analysis-DIRhFH6i-py3.11/lib/python3.11/site-packages/jax/_src/profiler.py:336\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    335\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m/mnt/storage/.cache/pypoetry/virtualenvs/emotion-analysis-DIRhFH6i-py3.11/lib/python3.11/site-packages/jax/_src/array.py:589\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated:\n\u001b[0;32m--> 589\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_device_array_to_np_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    for i, X in enumerate(dataloader['train']):\n",
    "        # Ignore padded entries\n",
    "        input_mask = X['conv_attn_mask'].astype(jnp.bool_)\n",
    "\n",
    "        # Forward and backward pass\n",
    "        loss_train, logits, key, trainer.state = trainer.train_step(key, trainer.state, X)\n",
    "\n",
    "        # Track training loss\n",
    "        print(i + 1, len(dataloader['train']), ': ', loss_train)\n",
    "\n",
    "        # Track F1\n",
    "        pred = logits[input_mask, :].argmax(axis=1)\n",
    "        real = X['emotion_labels'][input_mask]\n",
    "        f1_score.add_batch(predictions=pred, references=real)\n",
    "        accuracy.add_batch(predictions=pred, references=real)\n",
    "    print('train {}: {}  f1'.format(epoch, f1_score.compute(average='weighted')))\n",
    "    print('train {}: {} acc'.format(epoch, accuracy.compute()))\n",
    "\n",
    "    losses = []\n",
    "    for i, X in enumerate(dataloader['valid']):\n",
    "        # Ignore padded entries\n",
    "        input_mask = X['conv_attn_mask'].astype(jnp.bool_)\n",
    "\n",
    "        # Forward pass\n",
    "        loss_valid, logits, key, _ = trainer.eval_step(key, trainer.state, X)\n",
    "        losses.append(loss_valid)\n",
    "\n",
    "        # Track F1\n",
    "        pred = logits[input_mask, :].argmax(axis=1)\n",
    "        real = X['emotion_labels'][input_mask]\n",
    "        f1_score.add_batch(predictions=pred, references=real)\n",
    "        accuracy.add_batch(predictions=pred, references=real)\n",
    "    print('valid {}: {} loss'.format(epoch, np.array(losses).mean()))\n",
    "    print('valid {}: {}  f1'.format(epoch, f1_score.compute(average='weighted')))\n",
    "    print('valid {}: {} acc'.format(epoch, accuracy.compute()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-analysis-adVGJZXD-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
